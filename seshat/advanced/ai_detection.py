"""
AI-generated text detection.

Detects whether text was written by a human or generated by an AI/LLM.
Uses stylometric features that distinguish human and AI writing.
"""

from typing import Dict, List, Any, Optional
from collections import Counter
import numpy as np

from seshat.utils import tokenize_words, tokenize_sentences
from seshat.analyzer import Analyzer


class AIDetector:
    """
    Detect AI-generated vs human-written text.

    Based on research showing AI text has:
    - More consistent punctuation
    - Flatter vocabulary distribution
    - Missing cognitive load markers
    - More repetitive phrase structures
    """

    def __init__(self, analyzer: Optional[Analyzer] = None):
        """
        Initialize AI detector.

        Args:
            analyzer: Analyzer instance for feature extraction
        """
        self.analyzer = analyzer or Analyzer()
        self.cognitive_markers = self._define_cognitive_markers()
        self.hesitation_markers = self._define_hesitation_markers()

    def _define_cognitive_markers(self) -> List[str]:
        """Define markers of cognitive load (more common in human writing)."""
        return [
            "actually", "basically", "honestly", "literally", "obviously",
            "clearly", "simply", "essentially", "i mean", "you know",
            "like", "kind of", "sort of", "i think", "i guess",
            "well", "so", "anyway", "right", "okay",
        ]

    def _define_hesitation_markers(self) -> List[str]:
        """Define hesitation/filler markers (more common in human writing)."""
        return [
            "um", "uh", "er", "hmm", "huh", "well", "so",
            "like", "you know", "i mean", "basically", "actually",
        ]

    def detect(self, text: str) -> Dict[str, Any]:
        """
        Detect if text is AI-generated or human-written.

        Args:
            text: Input text to analyze

        Returns:
            Detection results with probability and indicators
        """
        if not text or len(text) < 100:
            return {
                "classification": "insufficient_text",
                "ai_probability": 0.5,
                "confidence": "low",
                "indicators": [],
                "human_markers": [],
                "ai_markers": [],
            }

        words = tokenize_words(text)
        sentences = tokenize_sentences(text)

        indicators = []
        human_markers = []
        ai_markers = []

        ai_score = 0.5

        cognitive_results = self._analyze_cognitive_markers(text, words)
        if cognitive_results["ratio"] < 0.01:
            ai_score += 0.1
            ai_markers.append("Low cognitive load markers")
        elif cognitive_results["ratio"] > 0.03:
            ai_score -= 0.1
            human_markers.append("Natural cognitive load markers present")

        consistency_results = self._analyze_consistency(text, words, sentences)
        if consistency_results["punctuation_consistency"] > 0.9:
            ai_score += 0.08
            ai_markers.append("Unnaturally consistent punctuation")

        if consistency_results["sentence_length_cv"] < 0.2:
            ai_score += 0.05
            ai_markers.append("Very consistent sentence lengths")

        vocab_results = self._analyze_vocabulary_distribution(words)
        if vocab_results["distribution_flatness"] > 0.7:
            ai_score += 0.1
            ai_markers.append("Flat vocabulary distribution")

        if vocab_results["hapax_ratio"] < 0.3:
            ai_score += 0.05
            ai_markers.append("Low hapax legomena ratio")

        repetition_results = self._analyze_repetition(text, sentences)
        if repetition_results["phrase_repetition"] > 0.15:
            ai_score += 0.1
            ai_markers.append("High phrase repetition")

        personality_results = self._analyze_personality_markers(text, words)
        if personality_results["emotional_inconsistency"]:
            ai_score += 0.05
            ai_markers.append("Emotional tone inconsistency")

        if personality_results["first_person_ratio"] > 0.03:
            ai_score -= 0.05
            human_markers.append("Natural first-person usage")

        ai_score = max(0, min(1, ai_score))

        if ai_score > 0.7:
            classification = "likely_ai"
            confidence = "high" if ai_score > 0.85 else "medium"
        elif ai_score > 0.55:
            classification = "possibly_ai"
            confidence = "low"
        elif ai_score < 0.3:
            classification = "likely_human"
            confidence = "high" if ai_score < 0.15 else "medium"
        elif ai_score < 0.45:
            classification = "possibly_human"
            confidence = "low"
        else:
            classification = "uncertain"
            confidence = "low"

        return {
            "classification": classification,
            "ai_probability": ai_score,
            "human_probability": 1 - ai_score,
            "confidence": confidence,
            "human_markers": human_markers,
            "ai_markers": ai_markers,
            "detailed_analysis": {
                "cognitive_markers": cognitive_results,
                "consistency": consistency_results,
                "vocabulary": vocab_results,
                "repetition": repetition_results,
                "personality": personality_results,
            },
        }

    def _analyze_cognitive_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze cognitive load markers."""
        text_lower = text.lower()
        word_count = len(words)

        marker_count = 0
        found_markers = []

        for marker in self.cognitive_markers:
            count = text_lower.count(marker)
            if count > 0:
                marker_count += count
                found_markers.append(marker)

        ratio = marker_count / word_count if word_count > 0 else 0

        return {
            "ratio": ratio,
            "count": marker_count,
            "found_markers": found_markers,
        }

    def _analyze_consistency(
        self, text: str, words: List[str], sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze unnatural consistency (AI tends to be too consistent)."""
        sentence_lengths = [len(s.split()) for s in sentences]

        if sentence_lengths:
            mean_length = np.mean(sentence_lengths)
            std_length = np.std(sentence_lengths)
            cv = std_length / mean_length if mean_length > 0 else 0
        else:
            cv = 0

        period_count = text.count(".")
        comma_count = text.count(",")

        if period_count > 0:
            comma_to_period = comma_count / period_count
        else:
            comma_to_period = 0

        punctuation_consistency = 0.5
        if 1.5 <= comma_to_period <= 2.5:
            punctuation_consistency = 0.9

        return {
            "sentence_length_cv": cv,
            "punctuation_consistency": punctuation_consistency,
            "comma_to_period_ratio": comma_to_period,
        }

    def _analyze_vocabulary_distribution(
        self, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze vocabulary distribution (AI tends to have flatter distribution)."""
        if not words:
            return {
                "distribution_flatness": 0.5,
                "hapax_ratio": 0.5,
            }

        word_counts = Counter(words)
        total_words = len(words)
        unique_words = len(word_counts)

        hapax = sum(1 for c in word_counts.values() if c == 1)
        hapax_ratio = hapax / unique_words if unique_words > 0 else 0

        frequencies = list(word_counts.values())
        max_freq = max(frequencies)
        normalized_freqs = [f / max_freq for f in frequencies]

        flatness = np.std(normalized_freqs)
        flatness_score = 1 - min(flatness, 1)

        return {
            "distribution_flatness": flatness_score,
            "hapax_ratio": hapax_ratio,
            "vocabulary_richness": unique_words / total_words if total_words > 0 else 0,
        }

    def _analyze_repetition(
        self, text: str, sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze phrase repetition patterns."""
        if len(sentences) < 3:
            return {"phrase_repetition": 0}

        sentence_starters = []
        for sentence in sentences:
            words = sentence.strip().split()[:3]
            if len(words) >= 2:
                sentence_starters.append(" ".join(words[:2]).lower())

        if sentence_starters:
            starter_counts = Counter(sentence_starters)
            repeated = sum(c - 1 for c in starter_counts.values() if c > 1)
            repetition_ratio = repeated / len(sentence_starters)
        else:
            repetition_ratio = 0

        return {
            "phrase_repetition": repetition_ratio,
        }

    def _analyze_personality_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze personality/emotional markers."""
        word_count = len(words)
        word_counts = Counter(words)

        first_person = sum(word_counts.get(w, 0) for w in ["i", "me", "my", "mine", "myself"])
        first_person_ratio = first_person / word_count if word_count > 0 else 0

        positive_words = ["happy", "great", "love", "amazing", "wonderful", "excellent"]
        negative_words = ["sad", "bad", "hate", "terrible", "awful", "horrible"]

        positive_count = sum(word_counts.get(w, 0) for w in positive_words)
        negative_count = sum(word_counts.get(w, 0) for w in negative_words)

        emotional_inconsistency = False

        return {
            "first_person_ratio": first_person_ratio,
            "emotional_inconsistency": emotional_inconsistency,
            "positive_ratio": positive_count / word_count if word_count > 0 else 0,
            "negative_ratio": negative_count / word_count if word_count > 0 else 0,
        }

    def detect_mixed(self, text: str, window_size: int = 200) -> Dict[str, Any]:
        """
        Detect if text contains a mix of AI and human writing.

        Useful for detecting AI-assisted writing.

        Args:
            text: Input text
            window_size: Words per analysis window

        Returns:
            Mixed detection results
        """
        words = text.split()

        if len(words) < window_size * 2:
            result = self.detect(text)
            return {
                "is_mixed": False,
                "overall": result,
                "segments": [],
            }

        segments = []
        for i in range(0, len(words) - window_size + 1, window_size // 2):
            segment_words = words[i:i + window_size]
            segment_text = " ".join(segment_words)

            result = self.detect(segment_text)
            segments.append({
                "start_word": i,
                "end_word": i + window_size,
                "ai_probability": result["ai_probability"],
                "classification": result["classification"],
            })

        ai_probs = [s["ai_probability"] for s in segments]
        variance = np.var(ai_probs)

        is_mixed = variance > 0.04

        return {
            "is_mixed": is_mixed,
            "variance": float(variance),
            "overall": self.detect(text),
            "segments": segments,
        }
