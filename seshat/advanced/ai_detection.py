"""
AI-generated text detection.

Detects whether text was written by a human or generated by an AI/LLM.
Uses stylometric features that distinguish human and AI writing.
"""

from typing import Dict, List, Any, Optional
from collections import Counter
import numpy as np

from seshat.utils import tokenize_words, tokenize_sentences
from seshat.analyzer import Analyzer


class AIDetector:
    """
    Detect AI-generated vs human-written text.

    Based on research showing AI text has:
    - More consistent punctuation
    - Flatter vocabulary distribution
    - Missing cognitive load markers
    - More repetitive phrase structures
    """

    def __init__(self, analyzer: Optional[Analyzer] = None):
        """
        Initialize AI detector.

        Args:
            analyzer: Analyzer instance for feature extraction
        """
        self.analyzer = analyzer or Analyzer()
        self.cognitive_markers = self._define_cognitive_markers()
        self.hesitation_markers = self._define_hesitation_markers()

    def _define_cognitive_markers(self) -> List[str]:
        """Define markers of cognitive load (more common in human writing)."""
        # Note: Exclude "so" as it's often used formally
        return [
            "actually", "basically", "honestly", "literally", "obviously",
            "clearly", "simply", "essentially", "i mean", "you know",
            "like", "kind of", "sort of", "i think", "i guess",
            "well", "anyway", "right", "okay", "sure", "yeah", "yep",
            "pretty much", "just like", "you see", "look", "listen",
        ]

    def _define_hesitation_markers(self) -> List[str]:
        """Define hesitation/filler markers (more common in human writing)."""
        return [
            "um", "uh", "er", "hmm", "huh", "well", "so",
            "like", "you know", "i mean", "basically", "actually",
        ]

    def detect(self, text: str) -> Dict[str, Any]:
        """
        Detect if text is AI-generated or human-written.

        Args:
            text: Input text to analyze

        Returns:
            Detection results with probability and indicators
        """
        if not text or len(text) < 100:
            return {
                "classification": "insufficient_text",
                "ai_probability": 0.5,
                "confidence": "low",
                "indicators": [],
                "human_markers": [],
                "ai_markers": [],
            }

        words = tokenize_words(text)
        sentences = tokenize_sentences(text)

        human_markers = []
        ai_markers = []

        # Start with slight AI bias for formal text (AI tends toward formality)
        ai_score = 0.5

        # Check for formality level - formal text starts with higher AI suspicion
        formality = self._analyze_formality(text, words)
        if formality["is_formal"]:
            ai_score += 0.15
            ai_markers.append("Formal academic tone")

        # Analyze cognitive load markers (filler words, hedging)
        cognitive_results = self._analyze_cognitive_markers(text, words)
        if cognitive_results["ratio"] < 0.005:
            ai_score += 0.15  # Increased from 0.1
            ai_markers.append("No cognitive load markers")
        elif cognitive_results["ratio"] < 0.01:
            ai_score += 0.08
            ai_markers.append("Low cognitive load markers")
        elif cognitive_results["ratio"] > 0.03:
            ai_score -= 0.15  # Increased from 0.1
            human_markers.append("Natural cognitive load markers present")

        # Analyze typos and errors (humans make typos, AI rarely does)
        error_results = self._analyze_errors(text, words)
        if error_results["has_typos"]:
            ai_score -= 0.12
            human_markers.append("Contains typos/errors")
        elif len(words) > 30:
            # Longer text with zero errors is suspicious
            ai_score += 0.12
            ai_markers.append("Perfect spelling/grammar in longer text")

        # Analyze sentence consistency
        consistency_results = self._analyze_consistency(text, words, sentences)
        if consistency_results["sentence_length_cv"] < 0.15:
            ai_score += 0.12  # Increased from 0.05
            ai_markers.append("Unnaturally consistent sentence lengths")
        elif consistency_results["sentence_length_cv"] > 0.5:
            ai_score -= 0.08
            human_markers.append("Natural sentence length variation")

        # Analyze vocabulary distribution (burstiness)
        vocab_results = self._analyze_vocabulary_distribution(words)
        if vocab_results["distribution_flatness"] > 0.75:
            ai_score += 0.12  # Increased from 0.1
            ai_markers.append("Flat vocabulary distribution")

        if vocab_results["hapax_ratio"] < 0.25:
            ai_score += 0.08  # Increased from 0.05
            ai_markers.append("Low hapax legomena ratio")

        # Analyze repetitive structures
        repetition_results = self._analyze_repetition(text, sentences)
        if repetition_results["phrase_repetition"] > 0.1:
            ai_score += 0.1
            ai_markers.append("Repetitive phrase structures")

        # Analyze transition patterns (AI uses more formal transitions)
        transition_results = self._analyze_transitions(text)
        if transition_results["formal_transition_ratio"] > 0.02:
            ai_score += 0.1
            ai_markers.append("Heavy use of formal transitions")

        # Analyze personality markers
        personality_results = self._analyze_personality_markers(text, words)
        # Only strong first-person (>8%) with informal tone suggests human
        if personality_results["first_person_ratio"] > 0.08 and not formality["is_formal"]:
            ai_score -= 0.08
            human_markers.append("Strong first-person voice")
        elif personality_results["first_person_ratio"] < 0.005:
            ai_score += 0.05
            ai_markers.append("Impersonal/third-person style")

        # Analyze contractions (humans use more contractions)
        contraction_results = self._analyze_contractions(text)
        if contraction_results["contraction_ratio"] > 0.02:
            ai_score -= 0.1
            human_markers.append("Natural contraction usage")
        elif contraction_results["contraction_ratio"] < 0.005 and len(words) > 50:
            ai_score += 0.08
            ai_markers.append("Avoids contractions")

        # Analyze AI-typical sentence patterns
        ai_patterns = self._analyze_ai_patterns(text)
        if ai_patterns["pattern_count"] >= 2:
            ai_score += 0.15
            ai_markers.append("Multiple AI-typical patterns detected")
        elif ai_patterns["pattern_count"] == 1:
            ai_score += 0.08
            ai_markers.append("AI-typical sentence pattern detected")

        ai_score = max(0, min(1, ai_score))

        # Classification thresholds
        if ai_score > 0.75:
            classification = "likely_ai"
            confidence = "high" if ai_score > 0.85 else "medium"
        elif ai_score > 0.6:
            classification = "possibly_ai"
            confidence = "medium" if ai_score > 0.68 else "low"
        elif ai_score < 0.25:
            classification = "likely_human"
            confidence = "high" if ai_score < 0.15 else "medium"
        elif ai_score < 0.4:
            classification = "possibly_human"
            confidence = "low"
        else:
            classification = "uncertain"
            confidence = "low"

        return {
            "classification": classification,
            "ai_probability": ai_score,
            "human_probability": 1 - ai_score,
            "confidence": confidence,
            "human_markers": human_markers,
            "ai_markers": ai_markers,
            "detailed_analysis": {
                "formality": formality,
                "cognitive_markers": cognitive_results,
                "errors": error_results,
                "consistency": consistency_results,
                "vocabulary": vocab_results,
                "repetition": repetition_results,
                "transitions": transition_results,
                "personality": personality_results,
                "contractions": contraction_results,
                "ai_patterns": ai_patterns,
            },
        }

    def _analyze_formality(self, text: str, words: List[str]) -> Dict[str, Any]:
        """Analyze text formality level."""
        formal_indicators = [
            "furthermore", "moreover", "consequently", "therefore", "thus",
            "additionally", "subsequently", "accordingly", "hence", "whereby",
            "nevertheless", "nonetheless", "regarding", "concerning", "pertaining",
            "demonstrates", "indicates", "suggests", "reveals", "establishes",
            "implementation", "utilization", "optimization", "methodology",
            "comprehensive", "fundamental", "significant", "substantial",
        ]

        text_lower = text.lower()
        formal_count = sum(1 for word in formal_indicators if word in text_lower)

        # Check for passive voice patterns
        passive_patterns = [" is ", " are ", " was ", " were ", " been ", " being "]
        passive_count = sum(text_lower.count(p) for p in passive_patterns)

        word_count = len(words)
        formal_ratio = formal_count / word_count if word_count > 0 else 0

        is_formal = formal_ratio > 0.01 or (passive_count > 3 and formal_count > 0)

        return {
            "is_formal": is_formal,
            "formal_word_count": formal_count,
            "formal_ratio": formal_ratio,
        }

    def _analyze_errors(self, text: str, words: List[str]) -> Dict[str, Any]:
        """Analyze typos and errors (humans make more)."""
        import re

        # Common typos - must be standalone words (word boundaries)
        typo_words = [
            "teh", "hte", "adn", "taht", "wiht", "recieve", "seperate",
            "occured", "untill", "definately", "accomodate", "occurence",
        ]

        # Informal contractions without apostrophes (typos/informal)
        informal_contractions = [
            "youre", "dont", "cant", "wont", "im", "id", "ive",
            "theyre", "theres", "heres", "wheres", "didnt", "doesnt",
            "wasnt", "werent", "isnt", "arent", "hasnt", "couldnt",
            "wouldnt", "shouldnt", "hadnt",
        ]

        # Informal spellings (intentional but human-like)
        informal_spellings = [
            "gonna", "wanna", "gotta", "kinda", "sorta", "lemme", "gimme",
            "cuz", "bcuz", "bc", "thru", "tho", "nite", "lite",
            "ur", "u", "r", "w/", "b4", "2day", "2nite",
        ]

        text_lower = text.lower()
        words_lower = [w.lower() for w in words]
        words_set = set(words_lower)

        # Check for typos as standalone words only
        typo_count = sum(1 for t in typo_words if t in words_set)

        # Check for informal contractions as standalone words
        informal_contraction_count = sum(1 for c in informal_contractions if c in words_set)

        # Check for informal spellings
        informal_count = sum(1 for i in informal_spellings if i in words_set)

        # Check for repeated characters (sooooo, nooooo) - must be 3+ repeats
        repeated_chars = len(re.findall(r'(.)\1{2,}', text_lower))

        # Check for multiple punctuation
        multi_punct = len(re.findall(r'[!?]{2,}|\.{3,}', text))

        total_errors = typo_count + informal_contraction_count + informal_count
        has_typos = (total_errors > 0 or repeated_chars > 0 or multi_punct > 0)

        return {
            "has_typos": has_typos,
            "typo_count": typo_count,
            "informal_contraction_count": informal_contraction_count,
            "informal_spelling_count": informal_count,
            "repeated_chars": repeated_chars,
            "multi_punctuation": multi_punct,
        }

    def _analyze_transitions(self, text: str) -> Dict[str, Any]:
        """Analyze transition word usage (AI overuses formal transitions)."""
        formal_transitions = [
            "furthermore", "moreover", "additionally", "consequently",
            "subsequently", "therefore", "thus", "hence", "accordingly",
            "in addition", "as a result", "for instance", "for example",
            "in conclusion", "to summarize", "in summary", "to conclude",
            "on the other hand", "in contrast", "conversely", "nevertheless",
            "nonetheless", "however", "although", "while", "whereas",
        ]

        text_lower = text.lower()
        word_count = len(text.split())

        transition_count = sum(1 for t in formal_transitions if t in text_lower)
        ratio = transition_count / word_count if word_count > 0 else 0

        return {
            "formal_transition_count": transition_count,
            "formal_transition_ratio": ratio,
        }

    def _analyze_ai_patterns(self, text: str) -> Dict[str, Any]:
        """Detect AI-typical sentence patterns and phrases."""
        import re

        # Common AI patterns - introductions
        ai_intro_patterns = [
            r"\bi believe that\b",
            r"\bit is important to note\b",
            r"\bit is worth noting\b",
            r"\bit should be noted\b",
            r"\bthis demonstrates\b",
            r"\bthis suggests\b",
            r"\bthis indicates\b",
            r"\bthis reveals\b",
            r"\bthis highlights\b",
            r"\bthis underscores\b",
        ]

        # AI conclusion patterns
        ai_conclusion_patterns = [
            r"\bin conclusion\b",
            r"\bto summarize\b",
            r"\bin summary\b",
            r"\bto conclude\b",
            r"\boverall,\b",
            r"\bin essence\b",
            r"\bultimately,\b",
        ]

        # AI filler phrases
        ai_filler_patterns = [
            r"\bthroughout (?:my|the|this)\b",
            r"\byields significant\b",
            r"\bleads to (?:personal|professional|significant)\b",
            r"\bconsistently achieve\b",
            r"\bfundamental to\b",
            r"\bdedicated individuals\b",
            r"\bpursuit of knowledge\b",
            r"\bpersonal growth\b",
            r"\bprofessional advancement\b",
        ]

        text_lower = text.lower()

        intro_count = sum(1 for p in ai_intro_patterns if re.search(p, text_lower))
        conclusion_count = sum(1 for p in ai_conclusion_patterns if re.search(p, text_lower))
        filler_count = sum(1 for p in ai_filler_patterns if re.search(p, text_lower))

        total = intro_count + conclusion_count + filler_count

        return {
            "pattern_count": total,
            "intro_patterns": intro_count,
            "conclusion_patterns": conclusion_count,
            "filler_patterns": filler_count,
        }

    def _analyze_contractions(self, text: str) -> Dict[str, Any]:
        """Analyze contraction usage (humans use more contractions)."""
        contractions = [
            "i'm", "i've", "i'll", "i'd", "you're", "you've", "you'll", "you'd",
            "he's", "she's", "it's", "we're", "we've", "we'll", "we'd",
            "they're", "they've", "they'll", "they'd", "that's", "there's",
            "here's", "what's", "who's", "how's", "where's", "when's",
            "isn't", "aren't", "wasn't", "weren't", "hasn't", "haven't",
            "hadn't", "doesn't", "don't", "didn't", "won't", "wouldn't",
            "couldn't", "shouldn't", "can't", "let's",
        ]

        text_lower = text.lower()
        word_count = len(text.split())

        contraction_count = sum(1 for c in contractions if c in text_lower)
        ratio = contraction_count / word_count if word_count > 0 else 0

        return {
            "contraction_count": contraction_count,
            "contraction_ratio": ratio,
        }

    def _analyze_cognitive_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze cognitive load markers."""
        text_lower = text.lower()
        word_count = len(words)

        marker_count = 0
        found_markers = []

        for marker in self.cognitive_markers:
            count = text_lower.count(marker)
            if count > 0:
                marker_count += count
                found_markers.append(marker)

        ratio = marker_count / word_count if word_count > 0 else 0

        return {
            "ratio": ratio,
            "count": marker_count,
            "found_markers": found_markers,
        }

    def _analyze_consistency(
        self, text: str, words: List[str], sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze unnatural consistency (AI tends to be too consistent)."""
        sentence_lengths = [len(s.split()) for s in sentences]

        if sentence_lengths:
            mean_length = np.mean(sentence_lengths)
            std_length = np.std(sentence_lengths)
            cv = std_length / mean_length if mean_length > 0 else 0
        else:
            cv = 0

        period_count = text.count(".")
        comma_count = text.count(",")

        if period_count > 0:
            comma_to_period = comma_count / period_count
        else:
            comma_to_period = 0

        punctuation_consistency = 0.5
        if 1.5 <= comma_to_period <= 2.5:
            punctuation_consistency = 0.9

        return {
            "sentence_length_cv": cv,
            "punctuation_consistency": punctuation_consistency,
            "comma_to_period_ratio": comma_to_period,
        }

    def _analyze_vocabulary_distribution(
        self, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze vocabulary distribution (AI tends to have flatter distribution)."""
        if not words:
            return {
                "distribution_flatness": 0.5,
                "hapax_ratio": 0.5,
            }

        word_counts = Counter(words)
        total_words = len(words)
        unique_words = len(word_counts)

        hapax = sum(1 for c in word_counts.values() if c == 1)
        hapax_ratio = hapax / unique_words if unique_words > 0 else 0

        frequencies = list(word_counts.values())
        max_freq = max(frequencies)
        normalized_freqs = [f / max_freq for f in frequencies]

        flatness = np.std(normalized_freqs)
        flatness_score = 1 - min(flatness, 1)

        return {
            "distribution_flatness": flatness_score,
            "hapax_ratio": hapax_ratio,
            "vocabulary_richness": unique_words / total_words if total_words > 0 else 0,
        }

    def _analyze_repetition(
        self, text: str, sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze phrase repetition patterns."""
        if len(sentences) < 3:
            return {"phrase_repetition": 0}

        sentence_starters = []
        for sentence in sentences:
            words = sentence.strip().split()[:3]
            if len(words) >= 2:
                sentence_starters.append(" ".join(words[:2]).lower())

        if sentence_starters:
            starter_counts = Counter(sentence_starters)
            repeated = sum(c - 1 for c in starter_counts.values() if c > 1)
            repetition_ratio = repeated / len(sentence_starters)
        else:
            repetition_ratio = 0

        return {
            "phrase_repetition": repetition_ratio,
        }

    def _analyze_personality_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze personality/emotional markers."""
        word_count = len(words)
        word_counts = Counter(words)

        first_person = sum(word_counts.get(w, 0) for w in ["i", "me", "my", "mine", "myself"])
        first_person_ratio = first_person / word_count if word_count > 0 else 0

        positive_words = ["happy", "great", "love", "amazing", "wonderful", "excellent"]
        negative_words = ["sad", "bad", "hate", "terrible", "awful", "horrible"]

        positive_count = sum(word_counts.get(w, 0) for w in positive_words)
        negative_count = sum(word_counts.get(w, 0) for w in negative_words)

        emotional_inconsistency = False

        return {
            "first_person_ratio": first_person_ratio,
            "emotional_inconsistency": emotional_inconsistency,
            "positive_ratio": positive_count / word_count if word_count > 0 else 0,
            "negative_ratio": negative_count / word_count if word_count > 0 else 0,
        }

    def detect_mixed(self, text: str, window_size: int = 200) -> Dict[str, Any]:
        """
        Detect if text contains a mix of AI and human writing.

        Useful for detecting AI-assisted writing.

        Args:
            text: Input text
            window_size: Words per analysis window

        Returns:
            Mixed detection results
        """
        words = text.split()

        if len(words) < window_size * 2:
            result = self.detect(text)
            return {
                "is_mixed": False,
                "overall": result,
                "segments": [],
            }

        segments = []
        for i in range(0, len(words) - window_size + 1, window_size // 2):
            segment_words = words[i:i + window_size]
            segment_text = " ".join(segment_words)

            result = self.detect(segment_text)
            segments.append({
                "start_word": i,
                "end_word": i + window_size,
                "ai_probability": result["ai_probability"],
                "classification": result["classification"],
            })

        ai_probs = [s["ai_probability"] for s in segments]
        variance = np.var(ai_probs)

        is_mixed = variance > 0.04

        return {
            "is_mixed": is_mixed,
            "variance": float(variance),
            "overall": self.detect(text),
            "segments": segments,
        }
