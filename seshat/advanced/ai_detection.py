"""
AI-generated text detection.

Detects whether text was written by a human or generated by an AI/LLM.
Uses stylometric features that distinguish human and AI writing.
"""

from typing import Dict, List, Any, Optional
from collections import Counter
import numpy as np

from seshat.utils import tokenize_words, tokenize_sentences
from seshat.analyzer import Analyzer


class AIDetector:
    """
    Detect AI-generated vs human-written text.

    Based on research showing AI text has:
    - More consistent punctuation
    - Flatter vocabulary distribution
    - Missing cognitive load markers
    - More repetitive phrase structures
    """

    def __init__(self, analyzer: Optional[Analyzer] = None):
        """
        Initialize AI detector.

        Args:
            analyzer: Analyzer instance for feature extraction
        """
        self.analyzer = analyzer or Analyzer()
        self.cognitive_markers = self._define_cognitive_markers()
        self.hesitation_markers = self._define_hesitation_markers()

    def _define_cognitive_markers(self) -> List[str]:
        """Define markers of cognitive load (more common in human writing)."""
        # Note: Exclude "so" as it's often used formally
        return [
            "actually", "basically", "honestly", "literally", "obviously",
            "clearly", "simply", "essentially", "i mean", "you know",
            "like", "kind of", "sort of", "i think", "i guess",
            "well", "anyway", "right", "okay", "sure", "yeah", "yep",
            "pretty much", "just like", "you see", "look", "listen",
        ]

    def _define_hesitation_markers(self) -> List[str]:
        """Define hesitation/filler markers (more common in human writing)."""
        return [
            "um", "uh", "er", "hmm", "huh", "well", "so",
            "like", "you know", "i mean", "basically", "actually",
        ]

    def detect(self, text: str) -> Dict[str, Any]:
        """
        Detect if text is AI-generated or human-written.

        Args:
            text: Input text to analyze

        Returns:
            Detection results with probability and indicators
        """
        if not text or len(text) < 100:
            return {
                "classification": "insufficient_text",
                "ai_probability": 0.5,
                "confidence": "low",
                "indicators": [],
                "human_markers": [],
                "ai_markers": [],
            }

        words = tokenize_words(text)
        sentences = tokenize_sentences(text)

        human_markers = []
        ai_markers = []

        # Start with slight AI bias for formal text (AI tends toward formality)
        ai_score = 0.5

        # Check for formality level - formal text has slight AI suspicion
        # (reduced from 0.15 - formal writing is common for humans too)
        formality = self._analyze_formality(text, words)
        if formality["is_formal"] and formality["formal_ratio"] > 0.03:
            ai_score += 0.08
            ai_markers.append("Highly formal academic tone")
        elif formality["is_formal"]:
            ai_score += 0.04
            ai_markers.append("Formal tone")

        # Analyze cognitive load markers (filler words, hedging)
        # Note: formal human writing also lacks these, so weight is reduced
        cognitive_results = self._analyze_cognitive_markers(text, words)
        if cognitive_results["ratio"] < 0.005:
            ai_score += 0.06  # Reduced - formal humans also avoid fillers
            ai_markers.append("No cognitive load markers")
        elif cognitive_results["ratio"] > 0.03:
            ai_score -= 0.15
            human_markers.append("Natural cognitive load markers present")
        elif cognitive_results["ratio"] > 0.015:
            ai_score -= 0.08
            human_markers.append("Some cognitive load markers")

        # Analyze typos and errors (humans make typos, AI rarely does)
        error_results = self._analyze_errors(text, words)
        if error_results["has_typos"]:
            ai_score -= 0.12
            human_markers.append("Contains typos/errors")
        elif len(words) > 30:
            # Longer text with zero errors is suspicious
            ai_score += 0.12
            ai_markers.append("Perfect spelling/grammar in longer text")

        # Analyze sentence consistency
        consistency_results = self._analyze_consistency(text, words, sentences)
        if consistency_results["sentence_length_cv"] < 0.15:
            ai_score += 0.12  # Increased from 0.05
            ai_markers.append("Unnaturally consistent sentence lengths")
        elif consistency_results["sentence_length_cv"] > 0.5:
            ai_score -= 0.08
            human_markers.append("Natural sentence length variation")

        # Analyze vocabulary distribution (burstiness)
        # Note: Short formal texts can also have flat distributions
        vocab_results = self._analyze_vocabulary_distribution(words)
        if vocab_results["distribution_flatness"] > 0.85 and len(words) > 50:
            ai_score += 0.08
            ai_markers.append("Very flat vocabulary distribution")
        elif vocab_results["distribution_flatness"] > 0.75 and len(words) > 100:
            ai_score += 0.05
            ai_markers.append("Flat vocabulary distribution")

        if vocab_results["hapax_ratio"] < 0.20 and len(words) > 50:
            ai_score += 0.05
            ai_markers.append("Low hapax legomena ratio")

        # Analyze repetitive structures
        repetition_results = self._analyze_repetition(text, sentences)
        if repetition_results["phrase_repetition"] > 0.1:
            ai_score += 0.1
            ai_markers.append("Repetitive phrase structures")

        # Analyze transition patterns (AI uses more formal transitions)
        # Note: Humans in academic/professional contexts also use transitions
        transition_results = self._analyze_transitions(text)
        if transition_results["formal_transition_ratio"] > 0.04:
            ai_score += 0.08
            ai_markers.append("Very heavy use of formal transitions")
        elif transition_results["formal_transition_ratio"] > 0.025:
            ai_score += 0.04
            ai_markers.append("Heavy formal transitions")

        # Analyze personality markers
        personality_results = self._analyze_personality_markers(text, words)
        # Only strong first-person (>8%) with informal tone suggests human
        if personality_results["first_person_ratio"] > 0.08 and not formality["is_formal"]:
            ai_score -= 0.08
            human_markers.append("Strong first-person voice")
        elif personality_results["first_person_ratio"] < 0.005:
            ai_score += 0.05
            ai_markers.append("Impersonal/third-person style")

        # Analyze contractions (humans use more contractions)
        contraction_results = self._analyze_contractions(text)
        if contraction_results["contraction_ratio"] > 0.02:
            ai_score -= 0.1
            human_markers.append("Natural contraction usage")
        elif contraction_results["contraction_ratio"] < 0.005 and len(words) > 50:
            ai_score += 0.08
            ai_markers.append("Avoids contractions")

        # Analyze AI-typical sentence patterns
        ai_patterns = self._analyze_ai_patterns(text)
        if ai_patterns["specific_ai_phrases"] >= 2:
            ai_score += 0.15
            ai_markers.append("Multiple AI-specific phrases detected")
        elif ai_patterns["specific_ai_phrases"] == 1:
            ai_score += 0.10
            ai_markers.append("AI-specific phrase detected")
        elif ai_patterns["pattern_count"] >= 2:
            ai_score += 0.05
            ai_markers.append("Multiple formal patterns")

        # Analyze human authenticity markers
        authenticity = self._analyze_human_authenticity(text)
        if authenticity["authenticity_score"] >= 3:
            ai_score -= 0.20
            human_markers.append("Strong human authenticity markers")
        elif authenticity["authenticity_score"] >= 2:
            ai_score -= 0.12
            human_markers.append("Human authenticity markers present")
        elif authenticity["authenticity_score"] >= 1:
            ai_score -= 0.06
            human_markers.append("Some human authenticity markers")

        ai_score = max(0, min(1, ai_score))

        # Classification thresholds
        if ai_score > 0.75:
            classification = "likely_ai"
            confidence = "high" if ai_score > 0.85 else "medium"
        elif ai_score > 0.6:
            classification = "possibly_ai"
            confidence = "medium" if ai_score > 0.68 else "low"
        elif ai_score < 0.25:
            classification = "likely_human"
            confidence = "high" if ai_score < 0.15 else "medium"
        elif ai_score < 0.4:
            classification = "possibly_human"
            confidence = "low"
        else:
            classification = "uncertain"
            confidence = "low"

        return {
            "classification": classification,
            "ai_probability": ai_score,
            "human_probability": 1 - ai_score,
            "confidence": confidence,
            "human_markers": human_markers,
            "ai_markers": ai_markers,
            "detailed_analysis": {
                "formality": formality,
                "cognitive_markers": cognitive_results,
                "errors": error_results,
                "consistency": consistency_results,
                "vocabulary": vocab_results,
                "repetition": repetition_results,
                "transitions": transition_results,
                "personality": personality_results,
                "contractions": contraction_results,
                "ai_patterns": ai_patterns,
                "authenticity": authenticity,
            },
        }

    def _analyze_formality(self, text: str, words: List[str]) -> Dict[str, Any]:
        """Analyze text formality level."""
        formal_indicators = [
            "furthermore", "moreover", "consequently", "therefore", "thus",
            "additionally", "subsequently", "accordingly", "hence", "whereby",
            "nevertheless", "nonetheless", "regarding", "concerning", "pertaining",
            "demonstrates", "indicates", "suggests", "reveals", "establishes",
            "implementation", "utilization", "optimization", "methodology",
            "comprehensive", "fundamental", "significant", "substantial",
        ]

        text_lower = text.lower()
        formal_count = sum(1 for word in formal_indicators if word in text_lower)

        # Check for passive voice patterns
        passive_patterns = [" is ", " are ", " was ", " were ", " been ", " being "]
        passive_count = sum(text_lower.count(p) for p in passive_patterns)

        word_count = len(words)
        formal_ratio = formal_count / word_count if word_count > 0 else 0

        is_formal = formal_ratio > 0.01 or (passive_count > 3 and formal_count > 0)

        return {
            "is_formal": is_formal,
            "formal_word_count": formal_count,
            "formal_ratio": formal_ratio,
        }

    def _analyze_errors(self, text: str, words: List[str]) -> Dict[str, Any]:
        """Analyze typos and errors (humans make more)."""
        import re

        # Common typos - must be standalone words (word boundaries)
        typo_words = [
            "teh", "hte", "adn", "taht", "wiht", "recieve", "seperate",
            "occured", "untill", "definately", "accomodate", "occurence",
        ]

        # Informal contractions without apostrophes (typos/informal)
        informal_contractions = [
            "youre", "dont", "cant", "wont", "im", "id", "ive",
            "theyre", "theres", "heres", "wheres", "didnt", "doesnt",
            "wasnt", "werent", "isnt", "arent", "hasnt", "couldnt",
            "wouldnt", "shouldnt", "hadnt",
        ]

        # Informal spellings (intentional but human-like)
        informal_spellings = [
            "gonna", "wanna", "gotta", "kinda", "sorta", "lemme", "gimme",
            "cuz", "bcuz", "bc", "thru", "tho", "nite", "lite",
            "ur", "u", "r", "w/", "b4", "2day", "2nite",
        ]

        text_lower = text.lower()
        words_lower = [w.lower() for w in words]
        words_set = set(words_lower)

        # Check for typos as standalone words only
        typo_count = sum(1 for t in typo_words if t in words_set)

        # Check for informal contractions as standalone words
        informal_contraction_count = sum(1 for c in informal_contractions if c in words_set)

        # Check for informal spellings
        informal_count = sum(1 for i in informal_spellings if i in words_set)

        # Check for repeated characters (sooooo, nooooo) - must be 3+ repeats
        repeated_chars = len(re.findall(r'(.)\1{2,}', text_lower))

        # Check for multiple punctuation
        multi_punct = len(re.findall(r'[!?]{2,}|\.{3,}', text))

        total_errors = typo_count + informal_contraction_count + informal_count
        has_typos = (total_errors > 0 or repeated_chars > 0 or multi_punct > 0)

        return {
            "has_typos": has_typos,
            "typo_count": typo_count,
            "informal_contraction_count": informal_contraction_count,
            "informal_spelling_count": informal_count,
            "repeated_chars": repeated_chars,
            "multi_punctuation": multi_punct,
        }

    def _analyze_transitions(self, text: str) -> Dict[str, Any]:
        """Analyze transition word usage (AI overuses formal transitions)."""
        formal_transitions = [
            "furthermore", "moreover", "additionally", "consequently",
            "subsequently", "therefore", "thus", "hence", "accordingly",
            "in addition", "as a result", "for instance", "for example",
            "in conclusion", "to summarize", "in summary", "to conclude",
            "on the other hand", "in contrast", "conversely", "nevertheless",
            "nonetheless", "however", "although", "while", "whereas",
        ]

        text_lower = text.lower()
        word_count = len(text.split())

        transition_count = sum(1 for t in formal_transitions if t in text_lower)
        ratio = transition_count / word_count if word_count > 0 else 0

        return {
            "formal_transition_count": transition_count,
            "formal_transition_ratio": ratio,
        }

    def _analyze_human_authenticity(self, text: str) -> Dict[str, Any]:
        """Detect markers of authentic human writing (even in formal contexts)."""
        import re

        authenticity_score = 0
        markers_found = []

        text_lower = text.lower()

        # Specific details (names, dates, places) - humans include specifics
        has_quoted_speech = bool(re.search(r'["\'].*?["\'].*?(?:said|told|asked|replied)', text))
        has_specific_names = bool(re.search(r'\b(?:Mr\.|Mrs\.|Ms\.|Dr\.)\s+[A-Z][a-z]+\b', text))
        has_specific_times = bool(re.search(r'\b(?:yesterday|last (?:week|month|year)|tomorrow|next (?:week|month))\b', text_lower))
        has_specific_numbers = bool(re.search(r'\b(?:\d{1,3}(?:,\d{3})*|\d+%|500|\$\d+)\b', text))

        if has_quoted_speech:
            authenticity_score += 1
            markers_found.append("quoted_speech")
        if has_specific_names:
            authenticity_score += 1
            markers_found.append("specific_names")
        if has_specific_times:
            authenticity_score += 1
            markers_found.append("specific_times")
        if has_specific_numbers:
            authenticity_score += 0.5
            markers_found.append("specific_numbers")

        # Self-correction and uncertainty (humans hedge and self-correct)
        hedging_patterns = [
            r"\bi'?m not (?:sure|certain)\b",
            r"\bmaybe\b",
            r"\bperhaps\b",
            r"\bi think\b",
            r"\bi guess\b",
            r"\bprobably\b",
            r"\bmight be\b",
            r"\bcould be\b",
            r"\bor rather\b",
            r"\bwell,? actually\b",
            r"\bi mean\b",
        ]
        hedging_count = sum(1 for p in hedging_patterns if re.search(p, text_lower))
        if hedging_count >= 2:
            authenticity_score += 1
            markers_found.append("hedging_language")
        elif hedging_count == 1:
            authenticity_score += 0.5
            markers_found.append("some_hedging")

        # Personal anecdotes and experiences
        anecdote_patterns = [
            r"\bwhen i was\b",
            r"\bi remember\b",
            r"\bonce i\b",
            r"\bmy (?:friend|colleague|boss|wife|husband|mom|dad)\b",
            r"\bi've (?:seen|heard|noticed|found)\b",
            r"\bhappened to me\b",
        ]
        anecdote_count = sum(1 for p in anecdote_patterns if re.search(p, text_lower))
        if anecdote_count >= 1:
            authenticity_score += 1
            markers_found.append("personal_anecdotes")

        # Conversational asides and parentheticals
        has_parenthetical = bool(re.search(r'\([^)]{5,50}\)', text))
        has_dash_aside = bool(re.search(r' - [^-]{5,50} - ', text))
        if has_parenthetical or has_dash_aside:
            authenticity_score += 0.5
            markers_found.append("parenthetical_asides")

        # Academic specifics (citations, methodology details)
        has_citation = bool(re.search(r'\(\d{4}\)|\bet al\.|\bfig(?:ure)?\.?\s*\d', text_lower))
        has_methodology = bool(re.search(r'\bwe (?:used|employed|conducted|analyzed|collected|measured|observed|found|examined|investigated|studied|surveyed|tested|sampled)\b', text_lower))
        has_data_reference = bool(re.search(r'\b(?:table|figure|appendix|section)\s+\d', text_lower))
        has_study_reference = bool(re.search(r'\b(?:this|our|the) (?:study|research|analysis|investigation|findings?|results?)\b', text_lower))
        if has_citation or has_methodology or has_data_reference:
            authenticity_score += 1
            markers_found.append("academic_specifics")
        elif has_study_reference:
            authenticity_score += 0.5
            markers_found.append("study_reference")

        # Acknowledging limitations or uncertainty (humans do this more)
        has_limitations = bool(re.search(r'\b(?:however|although|while|but|yet|still|despite)\b.*\b(?:limit|caveat|note|acknowledg|recogniz)\b', text_lower))
        if has_limitations:
            authenticity_score += 0.5
            markers_found.append("acknowledges_limitations")

        return {
            "authenticity_score": authenticity_score,
            "markers_found": markers_found,
        }

    def _analyze_ai_patterns(self, text: str) -> Dict[str, Any]:
        """Detect AI-typical sentence patterns and phrases."""
        import re

        # Highly specific AI phrases (rarely used by humans even formally)
        # These are very specific combinations that AI produces but humans rarely use
        ai_specific_phrases = [
            r"\byields significant (?:returns|benefits|results|improvements)\b",
            r"\bleads to (?:personal growth|professional advancement|significant)\b",
            r"\bdedicated individuals consistently\b",
            r"\bpursuit of knowledge leads\b",
            r"\bfundamental to (?:success|achieving|our)\b",
            r"\bcannot be (?:overstated|understated)\b",
            r"\bit is (?:important|essential|crucial|worth) (?:to note|noting) that\b",
            r"\bin today'?s (?:rapidly changing|fast-paced|modern|digital) world\b",
            r"\bdemonstrates? (?:our|a) commitment to\b",
            r"\bproactive measures to\b",
            r"\bboth a \w+ (?:imperative|necessity) and a \w+ (?:imperative|necessity)\b",
            r"\bembrace (?:sustainability|change|innovation)\b",
            r"\bmoral imperative\b",
            r"\bpractical necessity\b",
            r"\binvesting in \w+ yields\b",
            r"\bconsistently achieve their goals\b",
        ]

        # Generic patterns (common in formal writing, less weight)
        generic_formal_patterns = [
            r"\bin conclusion,?\b",
            r"\bfurthermore,?\b",
            r"\bthis demonstrates\b",
            r"\bthis suggests\b",
        ]

        text_lower = text.lower()

        # Count highly specific AI phrases (strong signal)
        specific_count = sum(1 for p in ai_specific_phrases if re.search(p, text_lower))

        # Count generic formal patterns (weak signal)
        generic_count = sum(1 for p in generic_formal_patterns if re.search(p, text_lower))

        # Only count generic patterns if there are also specific ones
        # or if there are many generic ones together
        effective_count = specific_count
        if specific_count > 0:
            effective_count += generic_count
        elif generic_count >= 3:
            effective_count = generic_count // 2  # Reduced weight

        return {
            "pattern_count": effective_count,
            "specific_ai_phrases": specific_count,
            "generic_formal_patterns": generic_count,
        }

    def _analyze_contractions(self, text: str) -> Dict[str, Any]:
        """Analyze contraction usage (humans use more contractions)."""
        contractions = [
            "i'm", "i've", "i'll", "i'd", "you're", "you've", "you'll", "you'd",
            "he's", "she's", "it's", "we're", "we've", "we'll", "we'd",
            "they're", "they've", "they'll", "they'd", "that's", "there's",
            "here's", "what's", "who's", "how's", "where's", "when's",
            "isn't", "aren't", "wasn't", "weren't", "hasn't", "haven't",
            "hadn't", "doesn't", "don't", "didn't", "won't", "wouldn't",
            "couldn't", "shouldn't", "can't", "let's",
        ]

        text_lower = text.lower()
        word_count = len(text.split())

        contraction_count = sum(1 for c in contractions if c in text_lower)
        ratio = contraction_count / word_count if word_count > 0 else 0

        return {
            "contraction_count": contraction_count,
            "contraction_ratio": ratio,
        }

    def _analyze_cognitive_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze cognitive load markers."""
        text_lower = text.lower()
        word_count = len(words)

        marker_count = 0
        found_markers = []

        for marker in self.cognitive_markers:
            count = text_lower.count(marker)
            if count > 0:
                marker_count += count
                found_markers.append(marker)

        ratio = marker_count / word_count if word_count > 0 else 0

        return {
            "ratio": ratio,
            "count": marker_count,
            "found_markers": found_markers,
        }

    def _analyze_consistency(
        self, text: str, words: List[str], sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze unnatural consistency (AI tends to be too consistent)."""
        sentence_lengths = [len(s.split()) for s in sentences]

        if sentence_lengths:
            mean_length = np.mean(sentence_lengths)
            std_length = np.std(sentence_lengths)
            cv = std_length / mean_length if mean_length > 0 else 0
        else:
            cv = 0

        period_count = text.count(".")
        comma_count = text.count(",")

        if period_count > 0:
            comma_to_period = comma_count / period_count
        else:
            comma_to_period = 0

        punctuation_consistency = 0.5
        if 1.5 <= comma_to_period <= 2.5:
            punctuation_consistency = 0.9

        return {
            "sentence_length_cv": cv,
            "punctuation_consistency": punctuation_consistency,
            "comma_to_period_ratio": comma_to_period,
        }

    def _analyze_vocabulary_distribution(
        self, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze vocabulary distribution (AI tends to have flatter distribution)."""
        if not words:
            return {
                "distribution_flatness": 0.5,
                "hapax_ratio": 0.5,
            }

        word_counts = Counter(words)
        total_words = len(words)
        unique_words = len(word_counts)

        hapax = sum(1 for c in word_counts.values() if c == 1)
        hapax_ratio = hapax / unique_words if unique_words > 0 else 0

        frequencies = list(word_counts.values())
        max_freq = max(frequencies)
        normalized_freqs = [f / max_freq for f in frequencies]

        flatness = np.std(normalized_freqs)
        flatness_score = 1 - min(flatness, 1)

        return {
            "distribution_flatness": flatness_score,
            "hapax_ratio": hapax_ratio,
            "vocabulary_richness": unique_words / total_words if total_words > 0 else 0,
        }

    def _analyze_repetition(
        self, text: str, sentences: List[str]
    ) -> Dict[str, Any]:
        """Analyze phrase repetition patterns."""
        if len(sentences) < 3:
            return {"phrase_repetition": 0}

        sentence_starters = []
        for sentence in sentences:
            words = sentence.strip().split()[:3]
            if len(words) >= 2:
                sentence_starters.append(" ".join(words[:2]).lower())

        if sentence_starters:
            starter_counts = Counter(sentence_starters)
            repeated = sum(c - 1 for c in starter_counts.values() if c > 1)
            repetition_ratio = repeated / len(sentence_starters)
        else:
            repetition_ratio = 0

        return {
            "phrase_repetition": repetition_ratio,
        }

    def _analyze_personality_markers(
        self, text: str, words: List[str]
    ) -> Dict[str, Any]:
        """Analyze personality/emotional markers."""
        word_count = len(words)
        word_counts = Counter(words)

        first_person = sum(word_counts.get(w, 0) for w in ["i", "me", "my", "mine", "myself"])
        first_person_ratio = first_person / word_count if word_count > 0 else 0

        positive_words = ["happy", "great", "love", "amazing", "wonderful", "excellent"]
        negative_words = ["sad", "bad", "hate", "terrible", "awful", "horrible"]

        positive_count = sum(word_counts.get(w, 0) for w in positive_words)
        negative_count = sum(word_counts.get(w, 0) for w in negative_words)

        emotional_inconsistency = False

        return {
            "first_person_ratio": first_person_ratio,
            "emotional_inconsistency": emotional_inconsistency,
            "positive_ratio": positive_count / word_count if word_count > 0 else 0,
            "negative_ratio": negative_count / word_count if word_count > 0 else 0,
        }

    def detect_mixed(self, text: str, window_size: int = 200) -> Dict[str, Any]:
        """
        Detect if text contains a mix of AI and human writing.

        Useful for detecting AI-assisted writing.

        Args:
            text: Input text
            window_size: Words per analysis window

        Returns:
            Mixed detection results
        """
        words = text.split()

        if len(words) < window_size * 2:
            result = self.detect(text)
            return {
                "is_mixed": False,
                "overall": result,
                "segments": [],
            }

        segments = []
        for i in range(0, len(words) - window_size + 1, window_size // 2):
            segment_words = words[i:i + window_size]
            segment_text = " ".join(segment_words)

            result = self.detect(segment_text)
            segments.append({
                "start_word": i,
                "end_word": i + window_size,
                "ai_probability": result["ai_probability"],
                "classification": result["classification"],
            })

        ai_probs = [s["ai_probability"] for s in segments]
        variance = np.var(ai_probs)

        is_mixed = variance > 0.04

        return {
            "is_mixed": is_mixed,
            "variance": float(variance),
            "overall": self.detect(text),
            "segments": segments,
        }
